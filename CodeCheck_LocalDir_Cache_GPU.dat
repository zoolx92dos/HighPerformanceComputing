# Under Interactive session for using GPUs
# this code is to check whether the models and datasets are being fetched from the local cache

import os
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM

local_cache = os.path.expanduser("~/hf_assets")
model_name = "Qwen/Qwen3-1.7B"

# Dataset from local cache
raw = load_dataset("ai4privacy/pii-masking-65k", cache_dir=local_cache)["train"].shuffle(seed=42)

# Tokenizer and model from local cache (no internet needed)
tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True, cache_dir=local_cache)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto",
    cache_dir=local_cache,
)


model_path = os.path.expanduser("~/hf_assets/models--Qwen--Qwen3-1.7B/snapshots/<commit_hash>")
models--Qwen--Qwen3-1.7B

model_path = os.path.expanduser("~/hf_assets/models--Qwen--Qwen3-1.7B/snapshots/<commit_hash>")

tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    torch_dtype="auto",
    device_map="auto"
)


# Downloading the models locally

# Make a directory for offline assets
mkdir -p ~/hf_assets

# 1. Download the dataset
python -c "from datasets import load_dataset; load_dataset('ai4privacy/pii-masking-65k', cache_dir='~/hf_assets')"

# 2. Download the model and tokenizer
python -c "from transformers import AutoTokenizer, AutoModelForCausalLM; \
AutoTokenizer.from_pretrained('Qwen/Qwen3-1.7B', cache_dir='~/hf_assets'); \
AutoModelForCausalLM.from_pretrained('Qwen/Qwen3-1.7B', cache_dir='~/hf_assets')"

(base) [username@hpcserver ~]$ export HF_DATASETS_OFFLINE=1
(base) [username@hpcserver ~]$ export TRANSFORMERS_OFFLINE=1
